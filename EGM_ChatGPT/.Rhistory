hist(thetaMAP$F1, col = "lightgray", main = paste("Histogram of MAP"),  xlab = NULL)
#2PL Model
Mod2 <-mirt(mydata, itemtype = '2PL')
#item parameters
Mod2_parameters<- coef(Mod2, simplify=TRUE,IRTpars=TRUE,  model = FALSE)
print(Mod2_parameters, digits = max(3L, getOption("digits") - 3L))
#plot all items
plot(Mod2, type = "trace", theta_lim = c(-4,4))
#plot select items that may be a concern
plot(Mod2, type = 'trace', which.items = c(2,6,8), facet_items=FALSE)
#Compare #3PL v 2PL
M2(Mod2,na.rm=TRUE)
M2(Mod3, na.rm = TRUE)
anova(Mod2,Mod3)
print(Mod2_parameters)
print(Mod3_parameters)
Mod1 <-mirt(mydata, itemtype = 'Rasch')
#item parameters
Mod1_parameters<- coef(Mod1, simplify=TRUE,IRTpars=TRUE)
print(Mod1_parameters)
#plot all items
plot(Mod1, type = "trace", theta_lim = c(-4,4))
#Model fit
M2(Mod1,na.rm=TRUE)
#Wright Map (1PL v Rasch????)
modd1 <- tam(mydata)
IRT.WrightMap(modd1)
anova(Mod1,Mod2,Mod3)#Mod2 Against Mod1,Mode3 Against Mod2
anova(Mod1,Mod2)
anova(Mod1,Mod3)
anova(Mod2,Mod3)
set.seed(123)
sample <- rnorm(n = 50, mean = 10, sd = 2)
set.seed(123)
sample <- rnorm(n = 51, mean = ((3.71+2.83)/2), sd = ((.47+.72)/2))
set.seed(123)
sample <- rnorm(n = 51, mean = ((3.71+2.83)/2), sd = ((.47+.72)/2))
# Calculate the standard error
sd(sample)/sqrt(50)
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
a_mean <- (3.71+2.83)/2
print(a_mean)
a_sd <- (.47+.72)/2
sample <- rnorm(n = 51, mean = a_mean, sd = a_sd)
mean(sample)
sd(sample)/sqrt(51)
sample <- rnorm(n = 51, mean = a_mean, sd = a_sd)
sd(sample)/sqrt(51)
install.packages(metafor)
install.packages("metafor")
install.packages("robvis")
install.packages("Tidyverse")
install.packages(Tidyverse)
library(tidyverse)
install.packages("tidyverse")
install.packages("ggThemeAssist")
install.packages("ggthemes")
# You'll only need the tidyverse library for this exercise
library(tidyverse)
library(scales)
library(forcats)
library(dplyr)
# Load original data
essential_raw <- read_csv("data/EssentialConstruction.csv")
# Clean the data a little
# Some of the borough names are in ALL CAPS, so we use str_to_title() to convert
# everything in the column to title case.
# We also make BOROUGH and CATEGORY factors (or categorical variables)
essential <- essential_raw %>%
mutate(BOROUGH = str_to_title(BOROUGH),
BOROUGH = factor(BOROUGH),
CATEGORY = factor(CATEGORY))
essential_by_borough <- essential %>%
group_by(BOROUGH) %>%
summarize(total = n()) %>%
mutate(proportion = total / sum(total)) %>%
mutate(nice_total = label_comma()(total)) %>%
mutate(BOROUGH = fct_reorder(BOROUGH, nice_total))
ggThemeAssist:::ggThemeAssistAddin()
ggThemeAssist:::ggThemeAssistAddin()
ggThemeAssist:::ggThemeAssistAddin()
ggThemeAssist:::ggThemeAssistAddin()
# Create a summarized dataset of projects by category
essential_by_category <- essential %>%
group_by(CATEGORY) %>%
summarize(total = n()) %>%
mutate(proportion = total / sum(total)) %>%
mutate(nice_total = label_comma()(total)) %>%
mutate(CATEGORY = fct_reorder(CATEGORY, nice_total))
# Add a lollipop chart here
ggplot(essential_by_category, aes(x = CATEGORY,  y = nice_total)) +
geom_point(color="skyblue") +
coord_flip() +
geom_segment(aes(x = CATEGORY, xend = CATEGORY,y = 0, yend = nice_total), color="#6F8FAF") +
geom_text(aes(label = nice_total), vjust = -1.5, size = 2,colour = "black") +
labs(x = " ", y = "Total",
title = "Construction Sites by Category",
subtitle = "Total number of constructions by type of construction") +
theme_classic() + theme(
legend.position = "none",
axis.text.y = element_blank(),
axis.title.x = element_text(
color = "gray",
size = 10,
face = "bold"),
axis.title.y = element_text(
color = "gray",
size = 10,
face = "bold")
)
ggThemeAssist:::ggThemeAssistAddin()
install.packages("remotes")
library(remotes)
install_github("elizagrames/litsearchr", ref="main")
library(swirl)
install.packages("swirl")
library(swirl)
swirl()
my_vector <- 1:20
my_vector
dim(my_vector)
length(my_vector)
dim(my_vector) <- c(4, 5)
0
matrix(31:60, nrow = 6, ncol = 5)
knitr::opts_chunk$set(echo = TRUE)
library(metafor)
# Read in a .csv file created in Excel
placement <- read.csv("placement.csv")
# We may also use a .RData file during the course of MATI
# you use the command load to open this file
# the name of the file has been encoded - here it is pd
load("math_pd.rdata")
View(pd)
?escalc
# We will use metafor for estimating the mean effect size
# We put our results into an object called placement_ma
# We will then need to call the object to see the results
placement_ma <- rma(yi = smd, # the variable with the ES
vi = varsmd, # the variable with the var(ES)
data = placement, # data set to use
method = "REML")  # method for RE model
# let's look at the names of variables in the data set with
# the function str (structure)
str(placement)
# To compute a standardized mean difference, we need the means, sds and sample
# sizes for each group
# You can learn more about it by typing "?escalc" on your console
# The relevant variables are sample sizes: MSTN and CntlN;
#   means: MSTmean and Cntlmean;
#   sds:  MSTsd and Cntlsd
# we will add the SMD and its variance to the placement data set
placement <- escalc(measure = "SMD",  # effect size type
n1i = MSTN,  # sample size for group 1
m1i = MSTmean, # mean for group 1
sd1i = MSTsd,  # sd for group 1
n2i = CntlN, # sample size for group 2
m2i = CntlN, # mean for group 2
sd2i = Cntlsd, # sd for group 2
data = placement, # data set name
var.names = c("smd", "varsmd")) # create names for smd and var
# We will use metafor for estimating the mean effect size
# We put our results into an object called placement_ma
# We will then need to call the object to see the results
placement_ma <- rma(yi = smd, # the variable with the ES
vi = varsmd, # the variable with the var(ES)
data = placement, # data set to use
method = "REML")  # method for RE model
# call the object to print it
placement_ma
# to produce a simple forest plot with an RE model
#  use the function forest.rma
# the input to the function is the object created by rma above
#  in our case it is placement_ma
forest.rma(placement_ma)
# to produce a simple forest plot with an RE model
#  use the function forest.rma
#  You can type ?forest.rma on your console to learn about the function
# the input to the function is the object created by rma above
#  in our case it is placement_ma
forest.rma(placement_ma)
library(metafor)
# Read in a .csv file created in Excel
placement <- read.csv("placement.csv")
# We may also use a .RData file during the course of MATI
# you use the command load to open this file
# the name of the file has been encoded - here it is pd
load("math_pd.rdata")
# let's look at the names of variables in the data set with
# the function str (structure)
str(placement)
# To compute a standardized mean difference, we need the means, sds and sample
# sizes for each group
# You can learn more about it by typing "?escalc" on your console
# The relevant variables are sample sizes: MSTN and CntlN;
#   means: MSTmean and Cntlmean;
#   sds:  MSTsd and Cntlsd
# we will add the SMD and its variance to the placement data set
placement <- escalc(measure = "SMD",  # effect size type
n1i = MSTN,  # sample size for group 1
m1i = MSTmean, # mean for group 1
sd1i = MSTsd,  # sd for group 1
n2i = CntlN, # sample size for group 2
m2i = CntlN, # mean for group 2
sd2i = Cntlsd, # sd for group 2
data = placement, # data set name
var.names = c("smd", "varsmd")) # create names for smd and var
# We will use metafor for estimating the mean effect size
# We put our results into an object called placement_ma
# We will then need to call the object to see the results
placement_ma <- rma(yi = smd, # the variable with the ES
vi = varsmd, # the variable with the var(ES)
data = placement, # data set to use
method = "REML")  # method for RE model
# call the object to print it
placement_ma
# to produce a simple forest plot with an RE model
#  use the function forest.rma
#  You can type ?forest.rma on your console to learn about the function
# the input to the function is the object created by rma above
#  in our case it is placement_ma
forest.rma(placement_ma)
knitr::opts_chunk$set(echo = TRUE)
# libraries for visualizations
library(naniar)
install.packages("naniae")
install.packages("naniar")
# libraries for visualizations
library(naniar)
library(visdat)
# code to extend libraries for meta-analysis
source("wrappers.R")
# library for data wrangling and plotting
library(tidyverse)
# read in data
adt_data <- readRDS("adt_data.RDS")
# summary of missing variables
gg_miss_var(adt_data) +
labs(y="Missing Cases",
title = "Total Missingness by Variable") +
theme(axis.title.x = element_text(size =10),
axis.text.y = element_text(size=6))
# summary of percent missing variables
gg_miss_var(adt_data, show_pct = T) +
labs(y="Percent Missing Cases",
title = "Percent Missingness by Variable") +
theme(axis.title.x = element_text(size =10),
axis.text.y = element_text(size=6))
# summary plots using vis_dat and vis_miss
gg_summary_covariate_miss(adt_data)
# use select to get a smaller data set
# the first entry is the data set
# the following entries are the variables I want
adt_data2 <- select(adt_data,
g2perwhite, g1numsessions, g2perhisp, g1perhisp,
g1perblack, g2perblack, g1hrsperweek, g2numsessions,
g2txdays, g2hrsperweek)
# this command is from naniar for an upset plot
# nsets is the number of variables to use
gg_miss_upset(adt_data2, nsets = 10, nintersects = NA)
# This function provides the percent missing for each variable
#  The "se_g" weights the percent missing by the SE of the ES
mis_ma_var_summary(adt_data, "se_g", truncate = TRUE)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
install.packages("devtools")
devtools::install_github("MikkelVembye/AIscreenR")
install.packages("devtools")
# Load packages
library(AIscreenR)
install.packages("AIScreenR")
devtools::install_github("MikkelVembye/AIscreenR")
library(AIscreenR)
library(revtools)
library(tibble)
library(dplyr)
# Installation if missing
#install.packages("devtools")
#devtools::install_github("MikkelVembye/AIscreenR")
# Load packages
library(AIscreenR)
library(revtools)
library(tibble)
library(dplyr)
# Installation if missing
#install.packages("devtools")
#devtools::install_github("MikkelVembye/AIscreenR")
# Load packages
library(AIscreenR)
library(revtools)
library(tibble)
library(dplyr)
# Installation if missing
#install.packages("devtools")
#devtools::install_github("MikkelVembye/AIscreenR")
# Load packages
library(AIscreenR)
library(revtools)
library(tibble)
library(purrr)
library(AIscreenR)
library(revtools)
library(tibble)
library(dplyr)
library(usethis)
library(future)
Screen_dat<- system.file("extdata", "Screening1_withResults", package = "AIscreenR")
Screen_dat<- system.file("EGM_ChatGPT", "Screening1_withResults", package = "AIscreenR")
Screen_dat<- system.file("EGM_ChatGPT", "Screening1_withResults.ris", package = "AIscreenR")
Screen_dat <- system.file("EGM_ChatGPT", "Screening1_withResults.ris", package = "AIscreenR")
Screen_dat <- system.file("EGM_ChatGPT", "Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = FALSE)
Screen_dat <- system.file("EGM_ChatGPT", "Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("Desktop", "Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("Desktop/EGM_ChatGPT", "Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file(..., "Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("","Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("","Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("","Screening1_withResults.RIS", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("","Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
system.file("")
system.file(".")
# Installation if missing
#install.packages("devtools")
#devtools::install_github("MikkelVembye/AIscreenR")
# Load packages
library(AIscreenR)
library(revtools)
library(tibble)
# library(dplyr) Removed due to error
# library(purrr) Removed due to error
library(usethis)
library(future)
Screen_dat <- system.file("","Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("/Home/Desktop/EGM_ChatGPT","Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
Screen_dat <- system.file("~/Desktop/EGM_ChatGPT","Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
setwd()
setwd(dir)
getwd()
Screen_dat <- system.file("/Users/carolinechubb/Desktop/EGM_ChatGPT","Screening1_withResults.ris", package = "AIscreenR", lib.loc = NULL,
mustWork = TRUE)
library(revtools)
screen_dat <- read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
View(screen_dat)
View(screen_dat)
View(screen_dat)
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
screening_data <- read_csv("Screening1_withResults.csv")
View(screen_dat)
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
decisions <- read_csv("customizations_log.csv")
library(readr)
installed.packages(readr)
library(readr)
library(readr)
decisions <- read_csv("customizations_log.csv")
# Installation if missing
#install.packages("devtools")
#devtools::install_github("MikkelVembye/AIscreenR")
# Load packages
library(AIscreenR)
library(revtools)
library(tibble)
# library(dplyr) Removed due to error
# library(purrr) Removed due to error
library(usethis)
library(future)
library(readr). # To use read_csv
# Installation if missing
install.packages("devtools")
devtools::install_github("MikkelVembye/AIscreenR")
# Load packages
library(AIscreenR)
library(revtools)
library(tibble)
# library(dplyr) Removed due to error
# library(purrr) Removed due to error
library(usethis)
library(future)
library(readr). # To use read_csv
install.packages("devtools")
# Installation if missing
install.packages("devtools")
devtools::install_github("MikkelVembye/AIscreenR")
# Load packages
library(AIscreenR)
library(revtools)
library(tibble)
# library(dplyr) Removed due to error
# library(purrr) Removed due to error
library(usethis)
library(future)
library(readr) # To use read_csv
install.packages("devtools")
1
# Load packages
library(AIscreenR)
library(revtools)
library(tibble)
# library(dplyr) Removed due to error
# library(purrr) Removed due to error
library(usethis)
library(future)
library(readr) # To use read_csv
# Load RIS file
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
# Decision Key
decisions <- read_csv("customizations_log.csv")
# Load RIS file
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
# Decision Key
decisions <- read_csv("customizations_log.csv")
View(decisions)
View(screen_dat)
decisions <- decisions %>%
group_by(article_id)
library(dplyr)
decisions <- decisions %>%
group_by(article_id)
decisions <- decisions %>%
group_by(article_id)
decisions <- read_csv("customizations_log.csv")
decisions <- decisions %>%
group_by(article_id)
decisions <- decisions %>%
group_by(article_id, user_id)
# Load RIS file
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")
# Decision Key
decisions <- read_csv("customizations_log.csv")
# Merge to include decisions
decisions <- decisions %>%
group_by(article_id, user_id, key)
# Rate limits across one model (Default is "gpt-3.5-turbo-0613")
rate_limits <- rate_limits_per_minute()
app_obj <-
approximate_price_gpt(
data = screen_dat, # RIS data
prompt = prompts,
studyid = studyid, # indicate the variable with the studyid in the data
title = title, # indicate the variable with the titles in the data
abstract = abstract, # indicate the variable with the abstracts in the data
model = c("gpt-3.5-turbo-0613", "gpt-3.5-turbo-0613", "gpt-4"),
reps = c(1, 10, 1),
top_p = c(0.001, 1)
)
prompt <- "Evaluate the following study based on the selection criteria for a systematic review on the use of R statistical programming in the course curriculum for applied research or statistics course at the college and university level. This search will include any studies that examine the use of R in an applied research course at the undergraduate or graduate level. This includes experimental, quasi experimental, observational, and qualitative studies. I would like to assess (1) Did the study include the R programming language? (2) Did the study take place in a college or university setting? (3) Does the study involve discipline(s) other than computer science? (4) Does the look at student experiences?"
app_obj <-
approximate_price_gpt(
data = screen_dat, # RIS data
prompt = prompts,
studyid = studyid, # indicate the variable with the studyid in the data
title = title, # indicate the variable with the titles in the data
abstract = abstract, # indicate the variable with the abstracts in the data
model = c("gpt-3.5-turbo-0613", "gpt-3.5-turbo-0613", "gpt-4"),
reps = c(1, 10, 1),
top_p = c(0.001, 1)
)
app_obj <-
approximate_price_gpt(
data = screen_dat, # RIS data
prompt = prompt,
studyid = studyid, # indicate the variable with the studyid in the data
title = title, # indicate the variable with the titles in the data
abstract = abstract, # indicate the variable with the abstracts in the data
model = c("gpt-3.5-turbo-0613", "gpt-3.5-turbo-0613", "gpt-4"),
reps = c(1, 10, 1),
top_p = c(0.001, 1)
)
app_obj <-
approximate_price_gpt(
data = screen_dat, # RIS data
prompt = prompt,
studyid = accession, # indicate the variable with the studyid in the data
title = title, # indicate the variable with the titles in the data
abstract = abstract, # indicate the variable with the abstracts in the data
model = c("gpt-3.5-turbo-0613", "gpt-3.5-turbo-0613", "gpt-4"),
reps = c(1, 10, 1),
top_p = c(0.001, 1)
)
app_obj
app_obj$price_dollar
app_obj$price_data
