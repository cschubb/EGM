---
title: "GPT_Screening"
author: "Caroline Sutton Chubb"
date: "2023-11-12"
output: html_document
---
Using ChatGPT for Title and Abstract Screening in Systematic Reviews
Source: [https://mikkelvembye.github.io/AIscreenR/articles/Using-ChatGPT-For-Screening.html]

# Preparation
```{r load-pacakges, message=FALSE}
# Installation if missing
#install.packages("devtools")
#devtools::install_github("MikkelVembye/AIscreenR")

# Load packages 
library(AIscreenR)
library(revtools)
library(tibble)
library(dplyr)
library(tidyverse)
# library(purrr) Removed due to error
library(usethis)
library(future)
library(readr) # To use read_csv

```

```{r prepare-files, warning=FALSE}
# Load RIS file
screen_dat <- revtools::read_bibliography("Screening1_withResults.ris")

# Decision Key
decisions <- read.csv("customizations_log.csv")

# Preview dataset as is

table(decisions$key)
table(decisions$value)
```
## Clean RIS file

```{r clean-RIS}

screen_dat <- screen_dat %>%
              # removes ""rayyan-" to match decisions csv file 
              mutate(accession = sub("rayyan-", "", accession)) %>%
              rename(article_id = accession) %>%
              mutate(article_id = as.character(article_id))
```


## Clean decisions

We found that the csv file from Rayyan was pulling a lot of extra information, including any time that labels were created or removed.  We wanted to make sure that our clean data only included the ultimate decisions to include, exclude, or maybe from both coders. 

Our cleaning decisions were based on [Rayyan's guidance one xporting references/articles in Rayyan] <https://help.rayyan.ai/hc/en-us/articles/4409161118737-Export-references-articles-in-Rayyan>.  

"Each action in the log file is logged with timestamp, user email, article ID and details of the action. The value after the included key determines the decision on the article (-1 exclude/ 0 maybe/ 1 include). While for labels/reasons, the key will be the label/reason and the value will show whether this label/reason was added (1), or deleted."

```{r clean-decisions, warning=FALSE}
# Clean dataset

decisions_clean <- decisions %>%
              select(created_at, user_id, article_id, key, value) %>%
              group_by(article_id, user_id, key) %>% 
  mutate(article_id = as.character(article_id)) %>%
           # convert time stamp format
  mutate(created_at = as.POSIXct(created_at, format = "%Y-%m-%dT%H:%M:%OS", tz = "UTC")) %>%    
  # rename by coder
              mutate(user_id = as.character(user_id),  # Convert user_id to character for case_when function
         user_id = case_when(
           user_id == "231089" ~ "CS",
           user_id == "296869" ~ "HS",
           TRUE ~ user_id  # Keeps other values as they are
         )) %>%
        rename(decision = value) %>%
        mutate(decision = as.character(decision), 
               decision = case_when(
                 decision == "-1" ~ "exclude", 
                  decision == "0" ~ "maybe",
                  decision == "1" ~ "include",
                 TRUE ~ decision)) %>%
  # removes all labels and notes
        filter(grepl("included", key)) %>%
  #Filter to keep only the most recent decision for each user
    group_by(user_id, article_id) %>%
  filter(created_at == max(created_at)) %>%
  ungroup()

table(decisions_clean$decision)

```

## Merge citations and decisions
Once all of the data was cleaned, we merged our citation RIS file with the decisions.  We reduced this to only include the columns we were most interested in. 

```{r merge}
merged_dataset <- decisions_clean %>%
  left_join(screen_dat, by = "article_id") %>%
  select(created_at, user_id, article_id, key, decision, type, title, year, journal, volume, issue, author, keywords, abstract)

```


## Create Prompt 
```{r prompt}
prompt <- "Evaluate the following study based on the selection criteria for a systematic review on the use of R statistical programming in the course curriculum for applied research or statistics course at the college and university level. This search will include any studies that examine the use of R in an applied research course at the undergraduate or graduate level. This includes experimental, quasi experimental, observational, and qualitative studies. I would like to assess (1) Did the study include the R programming language? (2) Did the study take place in a college or university setting? (3) Does the study involve discipline(s) other than computer science? (4) Does the look at student experiences?"
```

## Prepare for Test 

For testing purposes, we will be selecting 10 articles. This will help us avoid charges before the code is ready. 
```{r subset-test eval=FALSE}
test_articles <- c("1074364391", "1074364410", "1074364509", "1074364513", "1074364390")
test_data <- merged_dataset %>%
              filter(article_id %in% test_articles) %>%
              group_by(article_id)
```

## API Key Set Up

```{r eval=FALSE}

```

## Rate Limits
In order to calculate rate limits, you must set up API 
```{r rate-limits, eval=FALSE, include=FALSE}
# Rate limits across one model (Default is "gpt-3.5-turbo-0613")
rate_limits <- rate_limits_per_minute()
rate_limits


# Rate limits overview across multiple models
# Add further models if necessary
models <- c("gpt-3.5-turbo-0613", "gpt-4")

models_rate_limits <- rate_limits_per_minute(model = models) 
models_rate_limits
```
### Price
This price is an estimate of 100% Chat GPT screening.  Price may vary depending on ChatGPT version used. 
```{r API-price}
app_obj <- 
  approximate_price_gpt(
    data = merged_dataset, # RIS data
    prompt = prompt, 
    studyid = article_id, # indicate the variable with the studyid in the data
    title = title, # indicate the variable with the titles in the data
    abstract = abstract, # indicate the variable with the abstracts in the data
    model = c("gpt-3.5-turbo-0613", "gpt-3.5-turbo-0613", "gpt-4"),
    reps = c(1, 10, 1),
    top_p = c(0.001, 1)
  )

app_obj

app_obj$price_dollar
app_obj$price_data
```
# Screening titles and abstracts
```{r}

```

## Screening failed requests

## Getting the results from the screening

# Analyzing the screening

## Simple table

## False included and excluded (by gpt)
